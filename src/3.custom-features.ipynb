{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"},"colab":{"name":"3.custom-features.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"82OoOjJ2JAwT","colab_type":"text"},"source":["**Target Encoding :**\n","\n","https://www.kaggle.com/c/mercedes-benz-greener-manufacturing/discussion/36136#201638\n","\n","https://datascience.stackexchange.com/questions/11024/encoding-categorical-variables-using-likelihood-estimation\n","\n","https://www.kaggle.com/tnarik/likelihood-encoding-of-categorical-features\n"]},{"cell_type":"code","metadata":{"id":"FM9jpZ6baA_M","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":381},"executionInfo":{"status":"ok","timestamp":1593496115560,"user_tz":300,"elapsed":4023,"user":{"displayName":"Abhishek Reddy Palle","photoUrl":"","userId":"02455107033931856432"}},"outputId":"9c93c9ee-64b3-4fc4-df2f-26488cae0e30"},"source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n","  print('and then re-execute this cell.')\n","else:\n","  print(gpu_info)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Tue Jun 30 05:48:33 2020       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 450.36.06    Driver Version: 418.67       CUDA Version: 10.1     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   47C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n","|                               |                      |                 ERR! |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZHaXKADsZt9d","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":146},"executionInfo":{"status":"ok","timestamp":1593496137071,"user_tz":300,"elapsed":25509,"user":{"displayName":"Abhishek Reddy Palle","photoUrl":"","userId":"02455107033931856432"}},"outputId":"84e03cc6-5f54-4ee4-a618-80d9302b9fc1"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","%cd /content/gdrive/My Drive/M5-Evaluation"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n","/content/gdrive/.shortcut-targets-by-id/1IRMYDLHp5HGU8lqmV_ly3a9orQWGxVGB/M5-Evaluation\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0QexMJbhI-_F","colab_type":"code","colab":{}},"source":["## In this kernel I would like to show: \n","## 1. FE creation approaches\n","## 2. Sequential fe validation\n","## 3. Dimension reduction\n","## 4. FE validation by Permutation importance\n","## 5. Mean encodings\n","## 6. Parallelization for FE"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","id":"KgIk-gr8I-_K","colab_type":"code","colab":{}},"source":["import numpy as np \n","import pandas as pd \n","import os, sys, gc, warnings, psutil, random\n","\n","warnings.filterwarnings('ignore')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","id":"IpNed2LhI-_O","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":763},"executionInfo":{"status":"ok","timestamp":1593496234833,"user_tz":300,"elapsed":123225,"user":{"displayName":"Abhishek Reddy Palle","photoUrl":"","userId":"02455107033931856432"}},"outputId":"2b7fd709-0237-4476-8b10-9d4eca0d6e84"},"source":["########################### Load data\n","########################### Basic features were created here:\n","########################### https://www.kaggle.com/kyakovlev/m5-simple-fe\n","#################################################################################\n","\n","# Read data\n","grid_df = pd.concat([pd.read_pickle('./output/grid_part_1.pkl'),\n","                     pd.read_pickle('./output/grid_part_2.pkl').iloc[:,2:],\n","                     pd.read_pickle('./output/grid_part_3.pkl').iloc[:,2:]],\n","                     axis=1)\n","\n","# Subsampling\n","# to make all calculations faster.\n","# Keep only 5% of original ids.\n","keep_id = np.array_split(list(grid_df['id'].unique()), 20)[0]\n","grid_df = grid_df[grid_df['id'].isin(keep_id)].reset_index(drop=True)\n","\n","# Let's \"inspect\" our grid DataFrame\n","grid_df.info()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 3002725 entries, 0 to 3002724\n","Data columns (total 34 columns):\n"," #   Column            Dtype   \n","---  ------            -----   \n"," 0   id                category\n"," 1   item_id           category\n"," 2   dept_id           category\n"," 3   cat_id            category\n"," 4   store_id          category\n"," 5   state_id          category\n"," 6   d                 int16   \n"," 7   sales             float64 \n"," 8   release           int16   \n"," 9   sell_price        float16 \n"," 10  price_max         float16 \n"," 11  price_min         float16 \n"," 12  price_std         float16 \n"," 13  price_mean        float16 \n"," 14  price_norm        float16 \n"," 15  price_nunique     float16 \n"," 16  item_nunique      int16   \n"," 17  price_momentum    float16 \n"," 18  price_momentum_m  float16 \n"," 19  price_momentum_y  float16 \n"," 20  event_name_1      category\n"," 21  event_type_1      category\n"," 22  event_name_2      category\n"," 23  event_type_2      category\n"," 24  snap_CA           category\n"," 25  snap_TX           category\n"," 26  snap_WI           category\n"," 27  tm_d              int8    \n"," 28  tm_w              int8    \n"," 29  tm_m              int8    \n"," 30  tm_y              int8    \n"," 31  tm_wm             int8    \n"," 32  tm_dw             int8    \n"," 33  tm_w_end          int8    \n","dtypes: category(13), float16(10), float64(1), int16(3), int8(7)\n","memory usage: 162.0 MB\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8_NnDMzJhRVp","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":218},"executionInfo":{"status":"ok","timestamp":1593496234836,"user_tz":300,"elapsed":123208,"user":{"displayName":"Abhishek Reddy Palle","photoUrl":"","userId":"02455107033931856432"}},"outputId":"4d1ce768-e671-4ca2-8348-e7c47d6079b0"},"source":["grid_df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>item_id</th>\n","      <th>dept_id</th>\n","      <th>cat_id</th>\n","      <th>store_id</th>\n","      <th>state_id</th>\n","      <th>d</th>\n","      <th>sales</th>\n","      <th>release</th>\n","      <th>sell_price</th>\n","      <th>price_max</th>\n","      <th>price_min</th>\n","      <th>price_std</th>\n","      <th>price_mean</th>\n","      <th>price_norm</th>\n","      <th>price_nunique</th>\n","      <th>item_nunique</th>\n","      <th>price_momentum</th>\n","      <th>price_momentum_m</th>\n","      <th>price_momentum_y</th>\n","      <th>event_name_1</th>\n","      <th>event_type_1</th>\n","      <th>event_name_2</th>\n","      <th>event_type_2</th>\n","      <th>snap_CA</th>\n","      <th>snap_TX</th>\n","      <th>snap_WI</th>\n","      <th>tm_d</th>\n","      <th>tm_w</th>\n","      <th>tm_m</th>\n","      <th>tm_y</th>\n","      <th>tm_wm</th>\n","      <th>tm_dw</th>\n","      <th>tm_w_end</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>HOBBIES_1_008_CA_1_evaluation</td>\n","      <td>HOBBIES_1_008</td>\n","      <td>HOBBIES_1</td>\n","      <td>HOBBIES</td>\n","      <td>CA_1</td>\n","      <td>CA</td>\n","      <td>1</td>\n","      <td>12.0</td>\n","      <td>0</td>\n","      <td>0.459961</td>\n","      <td>0.500000</td>\n","      <td>0.419922</td>\n","      <td>0.019760</td>\n","      <td>0.476318</td>\n","      <td>0.919922</td>\n","      <td>4.0</td>\n","      <td>16</td>\n","      <td>NaN</td>\n","      <td>0.968750</td>\n","      <td>0.949219</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>29</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>5</td>\n","      <td>5</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>HOBBIES_1_009_CA_1_evaluation</td>\n","      <td>HOBBIES_1_009</td>\n","      <td>HOBBIES_1</td>\n","      <td>HOBBIES</td>\n","      <td>CA_1</td>\n","      <td>CA</td>\n","      <td>1</td>\n","      <td>2.0</td>\n","      <td>0</td>\n","      <td>1.559570</td>\n","      <td>1.769531</td>\n","      <td>1.559570</td>\n","      <td>0.032745</td>\n","      <td>1.764648</td>\n","      <td>0.881348</td>\n","      <td>2.0</td>\n","      <td>9</td>\n","      <td>NaN</td>\n","      <td>0.885742</td>\n","      <td>0.896484</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>29</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>5</td>\n","      <td>5</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>HOBBIES_1_010_CA_1_evaluation</td>\n","      <td>HOBBIES_1_010</td>\n","      <td>HOBBIES_1</td>\n","      <td>HOBBIES</td>\n","      <td>CA_1</td>\n","      <td>CA</td>\n","      <td>1</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>3.169922</td>\n","      <td>3.169922</td>\n","      <td>2.970703</td>\n","      <td>0.046356</td>\n","      <td>2.980469</td>\n","      <td>1.000000</td>\n","      <td>2.0</td>\n","      <td>20</td>\n","      <td>NaN</td>\n","      <td>1.064453</td>\n","      <td>1.043945</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>29</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>5</td>\n","      <td>5</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>HOBBIES_1_012_CA_1_evaluation</td>\n","      <td>HOBBIES_1_012</td>\n","      <td>HOBBIES_1</td>\n","      <td>HOBBIES</td>\n","      <td>CA_1</td>\n","      <td>CA</td>\n","      <td>1</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>5.980469</td>\n","      <td>6.519531</td>\n","      <td>5.980469</td>\n","      <td>0.115967</td>\n","      <td>6.468750</td>\n","      <td>0.916992</td>\n","      <td>3.0</td>\n","      <td>71</td>\n","      <td>NaN</td>\n","      <td>0.921875</td>\n","      <td>0.958984</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>29</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>5</td>\n","      <td>5</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>HOBBIES_1_015_CA_1_evaluation</td>\n","      <td>HOBBIES_1_015</td>\n","      <td>HOBBIES_1</td>\n","      <td>HOBBIES</td>\n","      <td>CA_1</td>\n","      <td>CA</td>\n","      <td>1</td>\n","      <td>4.0</td>\n","      <td>0</td>\n","      <td>0.700195</td>\n","      <td>0.720215</td>\n","      <td>0.680176</td>\n","      <td>0.011337</td>\n","      <td>0.706543</td>\n","      <td>0.972168</td>\n","      <td>3.0</td>\n","      <td>16</td>\n","      <td>NaN</td>\n","      <td>0.990234</td>\n","      <td>1.001953</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>29</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>5</td>\n","      <td>5</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                              id        item_id  ... tm_dw tm_w_end\n","0  HOBBIES_1_008_CA_1_evaluation  HOBBIES_1_008  ...     5        1\n","1  HOBBIES_1_009_CA_1_evaluation  HOBBIES_1_009  ...     5        1\n","2  HOBBIES_1_010_CA_1_evaluation  HOBBIES_1_010  ...     5        1\n","3  HOBBIES_1_012_CA_1_evaluation  HOBBIES_1_012  ...     5        1\n","4  HOBBIES_1_015_CA_1_evaluation  HOBBIES_1_015  ...     5        1\n","\n","[5 rows x 34 columns]"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"5Vp2UmwwI-_X","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":72},"executionInfo":{"status":"ok","timestamp":1593496361662,"user_tz":300,"elapsed":250010,"user":{"displayName":"Abhishek Reddy Palle","photoUrl":"","userId":"02455107033931856432"}},"outputId":"ec58c748-bb2f-4ec1-a185-7a92e327ef53"},"source":["########################### Baseline model\n","#################################################################################\n","\n","# We will need some global VARS for future\n","\n","SEED = 42             # Our random seed for everything\n","random.seed(SEED)     # to make all tests \"deterministic\"\n","np.random.seed(SEED)\n","N_CORES = psutil.cpu_count()     # Available CPU cores\n","\n","TARGET = 'sales'      # Our Target\n","END_TRAIN = 1941      # Last day in train set\n","\n","# Drop some items from \"TEST\" set part (1942...)\n","grid_df = grid_df[grid_df['d']<=END_TRAIN].reset_index(drop=True)\n","\n","# Features that we want to exclude from training\n","remove_features = ['id','d',TARGET]\n","\n","# Our baseline model serves\n","# to do fast checks of\n","# new features performance \n","\n","# We will use LightGBM for our tests\n","import lightgbm as lgb\n","lgb_params = {\n","                    'boosting_type': 'gbdt',         # Standart boosting type\n","                    'objective': 'regression',       # Standart loss for RMSE\n","                    'metric': ['rmse'],              # as we will use rmse as metric \"proxy\"\n","                    'subsample': 0.8,                \n","                    'subsample_freq': 1,\n","                    'learning_rate': 0.05,           # 0.5 is \"fast enough\" for us\n","                    'num_leaves': 2**7-1,            # We will need model only for fast check\n","                    'min_data_in_leaf': 2**8-1,      # So we want it to train faster even with drop in generalization \n","                    'feature_fraction': 0.8,\n","                    'n_estimators': 5000,            # We don't want to limit training (you can change 5000 to any big enough number)\n","                    'early_stopping_rounds': 30,     # We will stop training almost immediately (if it stops improving) \n","                    'seed': SEED,\n","                    'verbose': -1,\n","                } \n","\n","## RMSE\n","def rmse(y, y_pred):\n","    return np.sqrt(np.mean(np.square(y - y_pred)))\n","\n","# Small function to make fast features tests\n","# estimator = make_fast_test(grid_df)\n","# it will return lgb booster for future analisys\n","def make_fast_test(df):\n","\n","    features_columns = [col for col in list(df) if col not in remove_features]\n","\n","    tr_x, tr_y = df[df['d']<=(END_TRAIN-28)][features_columns], df[df['d']<=(END_TRAIN-28)][TARGET]              \n","    vl_x, v_y = df[df['d']>(END_TRAIN-28)][features_columns], df[df['d']>(END_TRAIN-28)][TARGET]\n","    \n","    train_data = lgb.Dataset(tr_x, label=tr_y)\n","    valid_data = lgb.Dataset(vl_x, label=v_y)\n","    \n","    estimator = lgb.train(\n","                            lgb_params,\n","                            train_data,\n","                            valid_sets = [train_data,valid_data],\n","                            verbose_eval = 500,\n","                        )\n","    \n","    return estimator\n","\n","# Make baseline model\n","baseline_model = make_fast_test(grid_df)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Training until validation scores don't improve for 30 rounds.\n","Early stopping, best iteration is:\n","[320]\ttraining's rmse: 2.82297\tvalid_1's rmse: 2.39415\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nRRRwUJhI-_Z","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":72},"executionInfo":{"status":"ok","timestamp":1593496576121,"user_tz":300,"elapsed":464444,"user":{"displayName":"Abhishek Reddy Palle","photoUrl":"","userId":"02455107033931856432"}},"outputId":"26446e17-506d-4ad8-fc6c-caf960df1fdc"},"source":["########################### Lets test our normal Lags (7 days)\n","########################### Some more info about lags here:\n","########################### https://www.kaggle.com/kyakovlev/m5-lags-features\n","#################################################################################\n","\n","# Small helper to make lags creation faster\n","from multiprocessing import Pool                # Multiprocess Runs\n","\n","## Multiprocessing Run.\n","# :t_split - int of lags days                   # type: int\n","# :func - Function to apply on each split       # type: python function\n","# This function is NOT 'bulletproof', be carefull and pass only correct types of variables.\n","## Multiprocess Runs\n","def df_parallelize_run(func, t_split):\n","    num_cores = np.min([N_CORES,len(t_split)])\n","    pool = Pool(num_cores)\n","    df = pd.concat(pool.map(func, t_split), axis=1)\n","    pool.close()\n","    pool.join()\n","    return df\n","\n","def make_normal_lag(lag_day):\n","    lag_df = grid_df[['id','d',TARGET]] # not good to use df from \"global space\"\n","    col_name = 'sales_lag_'+str(lag_day)\n","    lag_df[col_name] = lag_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(lag_day)).astype(np.float16)\n","    return lag_df[[col_name]]\n","\n","# Launch parallel lag creation\n","# and \"append\" to our grid\n","LAGS_SPLIT = [col for col in range(1,1+7)]\n","grid_df = pd.concat([grid_df, df_parallelize_run(make_normal_lag,LAGS_SPLIT)], axis=1)\n","\n","# Make features test\n","test_model = make_fast_test(grid_df)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Training until validation scores don't improve for 30 rounds.\n","Early stopping, best iteration is:\n","[163]\ttraining's rmse: 2.63602\tvalid_1's rmse: 2.26438\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vegPhCcTI-_e","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":508},"executionInfo":{"status":"ok","timestamp":1593496603697,"user_tz":300,"elapsed":491997,"user":{"displayName":"Abhishek Reddy Palle","photoUrl":"","userId":"02455107033931856432"}},"outputId":"540c5fb6-0d39-4d4f-8a65-82a8dc9dbd4d"},"source":["########################### Permutation importance Test\n","########################### https://www.kaggle.com/dansbecker/permutation-importance @dansbecker\n","#################################################################################\n","\n","# Let's creat validation dataset and features\n","features_columns = [col for col in list(grid_df) if col not in remove_features]\n","validation_df = grid_df[grid_df['d']>(END_TRAIN-28)].reset_index(drop=True)\n","\n","# Make normal prediction with our model and save score\n","validation_df['preds'] = test_model.predict(validation_df[features_columns])\n","base_score = rmse(validation_df[TARGET], validation_df['preds'])\n","print('Standart RMSE', base_score)\n","\n","\n","# Now we are looping over all our numerical features\n","for col in features_columns:\n","    \n","    # We will make validation set copy to restore\n","    # features states on each run\n","    temp_df = validation_df.copy()\n","    \n","    # Error here appears if we have \"categorical\" features and can't \n","    # do np.random.permutation without disrupt categories\n","    # so we need to check if feature is numerical\n","    if temp_df[col].dtypes.name != 'category':\n","        temp_df[col] = np.random.permutation(temp_df[col].values)\n","        temp_df['preds'] = test_model.predict(temp_df[features_columns])\n","        cur_score = rmse(temp_df[TARGET], temp_df['preds'])\n","        \n","        # If our current rmse score is less than base score\n","        # it means that feature most probably is a bad one\n","        # and our model is learning on noise\n","        print(col, np.round(cur_score - base_score, 4))\n","\n","# Remove Temp data\n","del temp_df, validation_df\n","\n","# Remove test features\n","# As we will compare performance with baseline model for now\n","keep_cols = [col for col in list(grid_df) if 'sales_lag_' not in col]\n","grid_df = grid_df[keep_cols]\n","\n","\n","# Results:\n","## Lags with 1 days shift (nearest past) are important\n","## Some other features are not important and probably just noise\n","## Better make several Permutation runs to confirm useless of the feature\n","## link again https://www.kaggle.com/dansbecker/permutation-importance @dansbecker\n","\n","## price_nunique -0.002 : strong negative values are most probably noise\n","## price_max -0.0002 : values close to 0 need deeper investigation\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Standart RMSE 2.26438289202465\n","release 0.0\n","sell_price 0.0035\n","price_max 0.001\n","price_min 0.0004\n","price_std 0.0014\n","price_mean 0.0024\n","price_norm 0.0068\n","price_nunique 0.0013\n","item_nunique 0.0013\n","price_momentum 0.0001\n","price_momentum_m 0.0039\n","price_momentum_y 0.0004\n","tm_d 0.0081\n","tm_w 0.0\n","tm_m 0.0002\n","tm_y 0.0\n","tm_wm 0.0002\n","tm_dw 0.1281\n","tm_w_end 0.0115\n","sales_lag_1 0.5943\n","sales_lag_2 0.0485\n","sales_lag_3 0.0191\n","sales_lag_4 0.017\n","sales_lag_5 0.0175\n","sales_lag_6 0.0213\n","sales_lag_7 0.0517\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nyOEoWQkI-_n","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":563},"executionInfo":{"status":"ok","timestamp":1593496888313,"user_tz":300,"elapsed":776591,"user":{"displayName":"Abhishek Reddy Palle","photoUrl":"","userId":"02455107033931856432"}},"outputId":"49b01a92-4acc-42f4-b59b-36871ac23b12"},"source":["########################### Lets test far away Lags (7 days with 56 days shift)\n","########################### and check permutation importance\n","#################################################################################\n","\n","LAGS_SPLIT = [col for col in range(56,56+7)]\n","grid_df = pd.concat([grid_df, df_parallelize_run(make_normal_lag,LAGS_SPLIT)], axis=1)\n","test_model = make_fast_test(grid_df)\n","\n","features_columns = [col for col in list(grid_df) if col not in remove_features]\n","validation_df = grid_df[grid_df['d']>(END_TRAIN-28)].reset_index(drop=True)\n","validation_df['preds'] = test_model.predict(validation_df[features_columns])\n","base_score = rmse(validation_df[TARGET], validation_df['preds'])\n","print('Standart RMSE', base_score)\n","\n","for col in features_columns:\n","    temp_df = validation_df.copy()\n","    if temp_df[col].dtypes.name != 'category':\n","        temp_df[col] = np.random.permutation(temp_df[col].values)\n","        temp_df['preds'] = test_model.predict(temp_df[features_columns])\n","        cur_score = rmse(temp_df[TARGET], temp_df['preds'])\n","        print(col, np.round(cur_score - base_score, 4))\n","\n","del temp_df, validation_df\n","        \n","# Remove test features\n","# As we will compare performance with baseline model for now\n","keep_cols = [col for col in list(grid_df) if 'sales_lag_' not in col]\n","grid_df = grid_df[keep_cols]\n","\n","\n","# Results:\n","## Lags with 56 days shift (far away past) are not as important\n","## as nearest past lags\n","## and at some point will be just noise for our model"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Training until validation scores don't improve for 30 rounds.\n","Early stopping, best iteration is:\n","[256]\ttraining's rmse: 2.86591\tvalid_1's rmse: 2.4084\n","Standart RMSE 2.408397679490214\n","release 0.0\n","sell_price 0.0199\n","price_max 0.0084\n","price_min 0.0053\n","price_std 0.0143\n","price_mean 0.0111\n","price_norm 0.0475\n","price_nunique 0.026\n","item_nunique 0.0065\n","price_momentum 0.0001\n","price_momentum_m 0.0281\n","price_momentum_y 0.0104\n","tm_d 0.006\n","tm_w 0.001\n","tm_m -0.0004\n","tm_y 0.0\n","tm_wm -0.0002\n","tm_dw 0.1169\n","tm_w_end 0.0115\n","sales_lag_56 0.0271\n","sales_lag_57 0.0113\n","sales_lag_58 0.0021\n","sales_lag_59 0.0035\n","sales_lag_60 0.0017\n","sales_lag_61 0.0048\n","sales_lag_62 0.0093\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"PRTXLe_NI-_q","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":145},"executionInfo":{"status":"ok","timestamp":1593497151480,"user_tz":300,"elapsed":1039731,"user":{"displayName":"Abhishek Reddy Palle","photoUrl":"","userId":"02455107033931856432"}},"outputId":"4bb209bc-6a4d-4ea5-ac3b-64ee1a1d1ab5"},"source":["########################### PCA\n","#################################################################################\n","\n","# The main question here - can we have \n","# almost same rmse boost with less features\n","# less dimensionality?\n","\n","# Lets try PCA and make 7->3 dimensionality reduction\n","\n","# PCA is \"unsupervised\" learning\n","# and with shifted target we can be sure\n","# that we have no Target leakage\n","from sklearn.decomposition import PCA\n","\n","def make_pca(df, pca_col, n_days):\n","    print('PCA:', pca_col, n_days)\n","    \n","    # We don't need any other columns to make pca\n","    pca_df = df[[pca_col,'d',TARGET]]\n","    \n","    # If we are doing pca for other series \"levels\" \n","    # we need to agg first\n","    if pca_col != 'id':\n","        merge_base = pca_df[[pca_col,'d']]\n","        pca_df = pca_df.groupby([pca_col,'d'])[TARGET].agg(['sum']).reset_index()\n","        pca_df[TARGET] = pca_df['sum']\n","        del pca_df['sum']\n","    \n","    # Min/Max scaling\n","    pca_df[TARGET] = pca_df[TARGET]/pca_df[TARGET].max()\n","    \n","    # Making \"lag\" in old way (not parallel)\n","    LAG_DAYS = [col for col in range(1,n_days+1)]\n","    format_s = '{}_pca_'+pca_col+str(n_days)+'_{}'\n","    pca_df = pca_df.assign(**{\n","            format_s.format(col, l): pca_df.groupby([pca_col])[col].transform(lambda x: x.shift(l))\n","            for l in LAG_DAYS\n","            for col in [TARGET]\n","        })\n","    \n","    pca_columns = list(pca_df)[3:]\n","    pca_df[pca_columns] = pca_df[pca_columns].fillna(0)\n","    pca = PCA(random_state=SEED)\n","    \n","    # You can use fit_transform here\n","    pca.fit(pca_df[pca_columns])\n","    pca_df[pca_columns] = pca.transform(pca_df[pca_columns])\n","    \n","    print(pca.explained_variance_ratio_)\n","    \n","    # we will keep only 3 most \"valuable\" columns/dimensions \n","    keep_cols = pca_columns[:3]\n","    print('Columns to keep:', keep_cols)\n","    \n","    # If we are doing pca for other series \"levels\"\n","    # we need merge back our results to merge_base df\n","    # and only than return resulted df\n","    # I'll skip that step here\n","    \n","    return pca_df[keep_cols]\n","\n","\n","# Make PCA\n","grid_df = pd.concat([grid_df, make_pca(grid_df,'id',7)], axis=1)\n","\n","# Make features test\n","test_model = make_fast_test(grid_df)\n","\n","# Remove test features\n","# As we will compare performance with baseline model for now\n","keep_cols = [col for col in list(grid_df) if '_pca_' not in col]\n","grid_df = grid_df[keep_cols]"],"execution_count":null,"outputs":[{"output_type":"stream","text":["PCA: id 7\n","[0.72224136 0.06621842 0.05938444 0.04201445 0.03891686 0.03614344\n"," 0.03508102]\n","Columns to keep: ['sales_pca_id7_1', 'sales_pca_id7_2', 'sales_pca_id7_3']\n","Training until validation scores don't improve for 30 rounds.\n","Early stopping, best iteration is:\n","[344]\ttraining's rmse: 2.62582\tvalid_1's rmse: 2.26603\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"FAGO6ToCI-_u","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":126},"executionInfo":{"status":"ok","timestamp":1593497276503,"user_tz":300,"elapsed":1164699,"user":{"displayName":"Abhishek Reddy Palle","photoUrl":"","userId":"02455107033931856432"}},"outputId":"bae48ef1-116a-4321-8fc7-4c579f479de4"},"source":["########################### Mean/std target encoding\n","#################################################################################\n","\n","# We will use these three columns for test\n","# (in combination with store_id)\n","icols = ['item_id','cat_id','dept_id']\n","\n","# But we can use any other column or even multiple groups\n","# like these ones\n","#            'state_id',\n","#            'store_id',\n","#            'cat_id',\n","#            'dept_id',\n","#            ['state_id', 'cat_id'],\n","#            ['state_id', 'dept_id'],\n","#            ['store_id', 'cat_id'],\n","#            ['store_id', 'dept_id'],\n","#            'item_id',\n","#            ['item_id', 'state_id'],\n","#            ['item_id', 'store_id']\n","\n","# There are several ways to do \"mean\" encoding\n","## K-fold scheme\n","## LOO (leave one out)\n","## Smoothed/regularized \n","## Expanding mean\n","## etc \n","\n","# You can test as many options as you want\n","# and decide what to use\n","# Because of memory issues you can't \n","# use many features.\n","\n","# We will use simple target encoding\n","# by std and mean agg\n","for col in icols:\n","    print('Encoding', col)\n","    temp_df = grid_df[grid_df['d']<=(1913-28)] # to be sure we don't have leakage in our validation set\n","    \n","    temp_df = temp_df.groupby([col,'store_id']).agg({TARGET: ['std','mean']})\n","    joiner = '_'+col+'_encoding_'\n","    temp_df.columns = [joiner.join(col).strip() for col in temp_df.columns.values]\n","    temp_df = temp_df.reset_index()\n","    grid_df = grid_df.merge(temp_df, on=[col,'store_id'], how='left')\n","    del temp_df\n","\n","# Make features test\n","test_model = make_fast_test(grid_df)\n","\n","# Remove test features\n","keep_cols = [col for col in list(grid_df) if '_encoding_' not in col]\n","grid_df = grid_df[keep_cols]\n","\n","# Bad thing that for some items  \n","# we are using past and future values.\n","# But we are looking for \"categorical\" similiarity\n","# on a \"long run\". So future here is not a big problem."],"execution_count":null,"outputs":[{"output_type":"stream","text":["Encoding item_id\n","Encoding cat_id\n","Encoding dept_id\n","Training until validation scores don't improve for 30 rounds.\n","Early stopping, best iteration is:\n","[295]\ttraining's rmse: 2.82223\tvalid_1's rmse: 2.40271\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Us9-ZQHFI-_z","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":72},"executionInfo":{"status":"ok","timestamp":1593497443184,"user_tz":300,"elapsed":1331351,"user":{"displayName":"Abhishek Reddy Palle","photoUrl":"","userId":"02455107033931856432"}},"outputId":"09abec4f-e704-4bfc-bcbc-7d8facfbaa67"},"source":["########################### Last non O sale\n","#################################################################################\n","\n","def find_last_sale(df,n_day):\n","    \n","    # Limit initial df\n","    ls_df = df[['id','d',TARGET]]\n","    \n","    # Convert target to binary\n","    ls_df['non_zero'] = (ls_df[TARGET]>0).astype(np.int8)\n","    \n","    # Make lags to prevent any leakage\n","    ls_df['non_zero_lag'] = ls_df.groupby(['id'])['non_zero'].transform(lambda x: x.shift(n_day).rolling(2000,1).sum()).fillna(-1)\n","\n","    temp_df = ls_df[['id','d','non_zero_lag']].drop_duplicates(subset=['id','non_zero_lag'])\n","    temp_df.columns = ['id','d_min','non_zero_lag']\n","\n","    ls_df = ls_df.merge(temp_df, on=['id','non_zero_lag'], how='left')\n","    ls_df['last_sale'] = ls_df['d'] - ls_df['d_min']\n","\n","    return ls_df[['last_sale']]\n","\n","\n","# Find last non zero\n","# Need some \"dances\" to fit in memory limit with groupers\n","grid_df = pd.concat([grid_df, find_last_sale(grid_df,1)], axis=1)\n","\n","# Make features test\n","test_model = make_fast_test(grid_df)\n","\n","# Remove test features\n","keep_cols = [col for col in list(grid_df) if 'last_sale' not in col]\n","grid_df = grid_df[keep_cols]"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Training until validation scores don't improve for 30 rounds.\n","Early stopping, best iteration is:\n","[341]\ttraining's rmse: 2.67719\tvalid_1's rmse: 2.29818\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fAZBywJ1r_nc","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":217},"executionInfo":{"status":"ok","timestamp":1593497528312,"user_tz":300,"elapsed":1416448,"user":{"displayName":"Abhishek Reddy Palle","photoUrl":"","userId":"02455107033931856432"}},"outputId":"6e80a309-1fa0-48ae-a370-0536dddcfeae"},"source":["########################### Apply on grid_df\n","#################################################################################\n","# lets read grid from \n","# https://www.kaggle.com/kyakovlev/m5-simple-fe\n","# to be sure that our grids are aligned by index\n","grid_df = pd.read_pickle('./output/grid_part_1.pkl')\n","grid_df[TARGET][grid_df['d']>(1913-28)] = np.nan\n","base_cols = list(grid_df)\n","\n","icols =  [\n","            ['state_id'],\n","            ['store_id'],\n","            ['cat_id'],\n","            ['dept_id'],\n","            ['state_id', 'cat_id'],\n","            ['state_id', 'dept_id'],\n","            ['store_id', 'cat_id'],\n","            ['store_id', 'dept_id'],\n","            ['item_id'],\n","            ['item_id', 'state_id'],\n","            ['item_id', 'store_id']\n","            ]\n","\n","for col in icols:\n","    print('Encoding', col)\n","    col_name = '_'+'_'.join(col)+'_'\n","    grid_df['enc'+col_name+'mean'] = grid_df.groupby(col)[TARGET].transform('mean').astype(np.float16)\n","    grid_df['enc'+col_name+'std'] = grid_df.groupby(col)[TARGET].transform('std').astype(np.float16)\n","\n","keep_cols = [col for col in list(grid_df) if col not in base_cols]\n","grid_df = grid_df[['id','d']+keep_cols]"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Encoding ['state_id']\n","Encoding ['store_id']\n","Encoding ['cat_id']\n","Encoding ['dept_id']\n","Encoding ['state_id', 'cat_id']\n","Encoding ['state_id', 'dept_id']\n","Encoding ['store_id', 'cat_id']\n","Encoding ['store_id', 'dept_id']\n","Encoding ['item_id']\n","Encoding ['item_id', 'state_id']\n","Encoding ['item_id', 'store_id']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"C838EAehI-_5","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1593497555634,"user_tz":300,"elapsed":1443748,"user":{"displayName":"Abhishek Reddy Palle","photoUrl":"","userId":"02455107033931856432"}},"outputId":"7023cc84-e699-438c-8aca-40fd5e3c0c04"},"source":["#################################################################################\n","print('Save Mean/Std encoding')\n","grid_df.to_pickle('./output/mean_encoding_df.pkl')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Save Mean/Std encoding\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Dw65GDL4I-_9","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":581},"executionInfo":{"status":"ok","timestamp":1593497555638,"user_tz":300,"elapsed":1443733,"user":{"displayName":"Abhishek Reddy Palle","photoUrl":"","userId":"02455107033931856432"}},"outputId":"ecb49ffe-f262-4cc9-95f6-a6e88a4ffc78"},"source":["########################### Final list of new features\n","#################################################################################\n","grid_df.info()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 47735397 entries, 0 to 47735396\n","Data columns (total 24 columns):\n"," #   Column                     Dtype   \n","---  ------                     -----   \n"," 0   id                         category\n"," 1   d                          int16   \n"," 2   enc_state_id_mean          float16 \n"," 3   enc_state_id_std           float16 \n"," 4   enc_store_id_mean          float16 \n"," 5   enc_store_id_std           float16 \n"," 6   enc_cat_id_mean            float16 \n"," 7   enc_cat_id_std             float16 \n"," 8   enc_dept_id_mean           float16 \n"," 9   enc_dept_id_std            float16 \n"," 10  enc_state_id_cat_id_mean   float16 \n"," 11  enc_state_id_cat_id_std    float16 \n"," 12  enc_state_id_dept_id_mean  float16 \n"," 13  enc_state_id_dept_id_std   float16 \n"," 14  enc_store_id_cat_id_mean   float16 \n"," 15  enc_store_id_cat_id_std    float16 \n"," 16  enc_store_id_dept_id_mean  float16 \n"," 17  enc_store_id_dept_id_std   float16 \n"," 18  enc_item_id_mean           float16 \n"," 19  enc_item_id_std            float16 \n"," 20  enc_item_id_state_id_mean  float16 \n"," 21  enc_item_id_state_id_std   float16 \n"," 22  enc_item_id_store_id_mean  float16 \n"," 23  enc_item_id_store_id_std   float16 \n","dtypes: category(1), float16(22), int16(1)\n","memory usage: 2.1 GB\n"],"name":"stdout"}]}]}